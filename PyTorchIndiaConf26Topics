1.
Mixture-of-Experts (MoE) architectures offer a powerful path to scaling model capacity without proportional increases in compute â€” but training them efficiently at large scale has traditionally required complex distributed systems expertise. This talk walks through how developers can scale MoE models directly in PyTorch, using distributed features and optimized parallelism strategies


2.
This talk will focus on the role of compilers in the era of AI programming frameworks (e.g. PyTorch) and AI hardware accelerators. AI models are evolving and continue to rely on high-performance computing. Specialized hardware for AI is often hard to program to exploit peak performance. AI models are also evolving in a way that is coupled with hardware strengths. This talk will describe how to build effective compiler systems in a layered way to improve hardware usability and deliver high performance as automatically as possible.

3.
Problem Framing:
Speech is the most natural interface for humansâ€”but historically the hardest modality to scale. In multilingual regions like India, text-first AI systems struggle for deep penetration making way for speech as the default gateway to equitable AI access.

Core Thesis:
ASR has transitioned from acoustic-first pipelines to an LLM-first paradigm. Modern decoder-only Audio LLMs treat speech as another tokenized modality, unlocking better multilingual scaling, reasoning, and adaptation.

What This Talk Covers:
Evolution of ASR architectures:
CTC â†’ Encoderâ€“Decoder (with cross-attention) â†’ Decoder-only Audio LLMs
Why alignment (audio â†’ token space) is the key technical unlock
Tradeoffs across pradigmns: latency, streaming, robustness, multilinguality
Practical post-training strategies for domain- and language-specific ASR
Why It Matters:
This shift collapses the boundary between speech recognition and language understandingâ€”making speech a first-class citizen in foundation model stacks.

4.beyond-the-brrr-building-a-unified-ecosystem-for-optimized-kernels
As deep learning models grow in complexity, the "efficiency tax" of generic operators has become a critical bottleneck. While custom kernels offer the promise of hardware-saturating performance, making the hardware go "brrr", the reality of implementing them is fraught with challenges. Developers are often caught in a cycle of managing device-specific dependencies, handling complex compilation chains, and struggling with the lack of modularity when swapping kernels across different hardware backends.
In this talk, we bridge the gap between low-level optimization and high-level developer productivity. We will explore why optimized kernels are essential for modern workloads and dissect the friction points that make them difficult to maintain and distribute. We will introduce the new Kernels library from Hugging Face, discussing how it makes the experience of working with pre-built kernels a seamless and enjoyable experience. Participants will learn how to discover, use, and share optimized kernels as easily as model checkpoints. They will also see how Kernels is integrated inside the mighty Transformers library and how it helps in shipping latency improvements.

5.
Deploying massive Mixture-of-Experts (MoE) models is primarily constrained by memory bandwidth and KV-cache fragmentation. This session presents an optimized inference pipeline integrating vLLMâ€™s PagedAttention and continuous batching with the native hardware acceleration of the NVIDIA Blackwell architecture. We will demonstrate the implementation of native NVFP4 quantization on Blackwell Tensor Cores, analyzing how 4-bit floating-point precision on Blackwell mitigates memory bandwidth bottlenecks to accelerate tensor execution and maximize token throughput for sparse architectures.


===========
questions to pose:
======
Core Resources to Understand Topic #1 (MoE in PyTorch)
ğŸ§  1) Foundational Papers

â€œMixture of Experts with Shardingâ€ â€” Google / GShard (introduces MoE scaling)
â¤ Understand expert routing and conditional computation.

â€œSwitch Transformers: Scaling to Trillion Parameter Modelsâ€
â¤ One of the most influential MoE models.

â€œFast MoE: Efficient Sparsely Activated Transformersâ€
â¤ Optimization-oriented strategies relevant to PyTorch scaling.

These give the theory â€” gating, expert balancing, capacity factor, load balancing loss.

ğŸ›  2) PyTorch & MoE Training Tutorials

PyTorch MoE Example Repos

Facebook / PyTorch MoE implementations

Examples in torch.distributed (RPC based)

DeepSpeed MoE Tutorials

How ZeRO + MoE jointly scale training

Expert balancing & partitioning

DeepSpeed implementation details help bridge theory â†’ practical execution.

ğŸ§ª 3) Lecture / Video Resources

MIT Deep Learning Lecture: Efficient Models

Mixture of Experts intro from first principles

YouTube talks

â€œMoE breakdownâ€ sessions by Hugging Face, NVIDIA GTC talks

Practical PyTorch implementations

ğŸ“˜ 4) Blog / Conceptual Writeups

Distill Pub â€” MoE visual explanations

Great for intuition around how gates pick experts

Hugging Face & DeepSpeed blogs

Step-by-step training considerations

ğŸ” Key Concepts You Should Master Before the Talk

ğŸ“Œ Expert routing & gate design
ğŸ“Œ Load balancing & capacity factor
ğŸ“Œ Distributed training overheads
ğŸ“Œ Memory & communication bottlenecks
ğŸ“Œ Comparison with dense models

ğŸ§  5 High-Impact Questions to Ask at the Talk

These arenâ€™t generic â€” theyâ€™re crafted to show systems-level depth and practical engineering insight.

â“ Question 1 â€” Load Balancing

â€œHow does your implementation handle expert capacity imbalance at scale, and what load balancing losses or strategies do you use to prevent expert underutilization?â€

ğŸ’¡ Why itâ€™s impressive
MoE systems break if experts arenâ€™t balanced â€” this is where many go wrong.

âœ… Strong Answer

â€œWe use a combination of auxiliary load-balancing loss terms and capacity factors. By using a softmax gating with additional balancing regularizers, we discourage the gate from collapsing to a few experts. In PyTorch distributed, we also shard experts so that each device holds multiple experts, which reduces imbalance across machines.â€

â“ Question 2 â€” Communication Overhead

â€œIn your distributed design, where do you see most of the communication overhead â€” expert updates or routing â€” and how do you mitigate that?â€

ğŸ’¡ Why itâ€™s impressive
Recruiters want to see you think about the hidden cost of distribution.

âœ… Strong Answer

â€œThe dominant overhead usually comes from all-to-all communication during expert dispatch. To mitigate this, the implementation balances local expert replicas and uses efficient collective operations. When possible, we fuse communication with forward/backward passes, reducing transfer overhead between GPUs.â€

â“ Question 3 â€” Scalability Limits

â€œAt what scale (experts Ã— batch size Ã— devices) did you observe diminishing returns, and what were the system bottlenecks?â€

ğŸ’¡ Why itâ€™s impressive
Shows you think about engineering ceilings, not just theory.

âœ… Strong Answer

â€œWe saw diminishing returns when communication and KV cache overheads exceeded compute cost â€” around 128 experts with small batch sizes. At that point, routing overhead dominates. The threshold depends on batch size, expert count, and the quality of the collective backend (NCCL / Gloo).â€

â“ Question 4 â€” Model Accuracy Stability

â€œDid you observe any pattern where increasing experts hurt convergence stability, and if so, what mitigation strategies did you adopt?â€

ğŸ’¡ Why itâ€™s impressive
Connects system scaling with training quality.

âœ… Strong Answer

â€œYes â€” as expert count grows, the gate can become unstable. We used warm-up schedules for gate weights, gradient clipping on gating parameters, and occasionally auxiliary losses to stabilize routes. Normalizing expert outputs also helped.â€

â“ Question 5 â€” Memory & Gradient Overheads

â€œWhat memory optimizations did you require in PyTorch â€” like gradient checkpointing or expert sharding â€” to handle tens of millions of parameters?â€

ğŸ’¡ Why itâ€™s impressive
Shows you care about actual capacity limits.

âœ… Strong Answer

â€œWe heavily rely on expert sharding and checkpointing for intermediate activations. By sharding expert weights across devices, we reduce peak memory. Checkpointing the expert forward pass saves activation memory at the cost of recompute, which was balanced with batch size selection.â€

ğŸ“Œ Bonus: 3 Things to Say Before Asking Your Question

You want to frame yourself as a peer â€” not someone fishing for an answer.

Try these openers:

Before Q1

â€œIâ€™ve seen load imbalance crash MoE training at scale â€” Iâ€™m curious how you approach it.â€

Before Q2

â€œIn my experience with distributed workloads, communication trumps compute â€” how do you see it in MoE?â€

Before Q5

â€œMemory fragmentation becomes critical when sharding â€” have you run into that?â€

ğŸ“ˆ How This Positions You to Stakeholders

âœ” Recruiters: You immediately sound like someone familiar with real system constraints
âœ” Engineering Leaders: You show systems maturity, not just model theory
âœ” Open-Source Maintainers: You ask questions that could inform future frameworks

======
TOPICS MUST COVER:
====

Perfect ğŸ‘
Hereâ€™s a **tight, no-fluff 20-minute deep-prep plan** for **Topic 1: Scaling Mixture-of-Experts (MoE) in PyTorch**, optimized so you can **listen to the talk like an insider** and ask questions that *land*.

Iâ€™ve structured this like a **Staff+ engineer cram guide** â€” what to listen to, what to skim, and *what to mentally anchor* while listening.

---

# â±ï¸ 20-Minute MoE Mastery Plan (Talk-Ready)

## ğŸ§  Minute 0â€“5: Core Mental Model (NON-NEGOTIABLE)

**Goal:** Understand *why* MoE exists and where it breaks.

### ğŸ§ Listen / Read (pick 1)

* **Switch Transformers overview talk** (YouTube â€“ Google Research)
* **Hugging Face blog: â€œMixture of Experts Explainedâ€**

### Anchor these ideas in your head:

* MoE = **conditional computation** (only few experts active per token)
* Scaling benefit comes from **parameter count â‰  FLOPs**
* Core components:

  * **Gate** (router)
  * **Experts** (FFNs)
  * **Capacity factor**
  * **Load balancing loss**

ğŸ‘‰ If you can explain *why MoE is cheaper than dense models* in 2 sentences, move on.

---

## ğŸ§± Minute 5â€“10: PyTorch + Distributed Reality

**Goal:** Understand *how this actually runs* in PyTorch.

### ğŸ§ Listen / Skim

* **DeepSpeed MoE tutorial (video or blog)**
* PyTorch Distributed overview (focus on `all_to_all`)

### Focus on:

* Why **all-to-all communication** exists in MoE
* Experts are often **sharded across GPUs**
* Forward pass = route â†’ dispatch â†’ compute â†’ gather

### Key realization (important for questions):

> MoE training is often **communication-bound, not compute-bound**

---

## âš™ï¸ Minute 10â€“15: Failure Modes & Scaling Limits

**Goal:** Know what *breaks* so your questions sound real.

### ğŸ§ Read / Watch

* **FastMoE paper summary blog**
* Any talk mentioning â€œexpert collapseâ€ or â€œimbalanceâ€

### Lock in these failure modes:

* **Expert collapse** (gate sends everything to 1â€“2 experts)
* **Underutilized GPUs** despite massive models
* **Batch size sensitivity**
* **Routing instability early in training**

If the speaker mentions *any* of these â€” youâ€™re aligned.

---

## ğŸ§ª Minute 15â€“20: Optimization Levers (This is where you impress)

**Goal:** Connect model â†’ system â†’ cost.

### ğŸ§ Skim

* DeepSpeed MoE optimization section
* Blogs mentioning:

  * Capacity factor tuning
  * Auxiliary load balancing loss
  * Expert replication vs sharding

### Mental checklist:

* Load balancing loss â‰  optional
* Capacity factor is a **latency vs utilization tradeoff**
* Bigger MoE â‰  better unless routing is stable

---

# ğŸ¯ 5 MUST-ASK QUESTIONS (WITH ANSWERS YOU SHOULD KNOW)

These are **talk-safe** (wonâ€™t stump speakers) but **audience-impressive**.

---

## â“ Q1 â€” Load Balancing (Top-Tier Question)

**Question**

> *â€œHow do you prevent expert collapse at scale, and what role does the load-balancing loss play relative to capacity factor tuning?â€*

**What a good answer sounds like**

> â€œWe use an auxiliary load-balancing loss to encourage uniform routing across experts, combined with a capacity factor that limits how many tokens an expert can accept. The balance is important â€” too low capacity increases token dropping, too high reduces sparsity benefits.â€

**Why this impresses**

* Shows you understand **training dynamics + systems**

---

## â“ Q2 â€” Communication Bottleneck

**Question**

> *â€œIn your PyTorch implementation, does all-to-all communication dominate runtime at scale, and how do you mitigate it?â€*

**Expected answer**

> â€œYes, all-to-all often dominates. We mitigate it by expert sharding, overlapping communication with compute, and ensuring sufficient batch size so routing overhead amortizes.â€

**Signal**

* Youâ€™re thinking like a **distributed systems engineer**

---

## â“ Q3 â€” Scaling Ceiling

**Question**

> *â€œDid you observe a point where increasing the number of experts stopped improving throughput or convergence?â€*

**Expected answer**

> â€œYes â€” beyond a certain expert count, routing and communication overhead outweigh compute savings, especially with small batch sizes. Scaling only helps when batch size and hardware topology support it.â€

**Signal**

* You think in **engineering limits**, not hype

---

## â“ Q4 â€” Training Stability

**Question**

> *â€œHow sensitive was convergence to gate initialization and early training dynamics?â€*

**Expected answer**

> â€œVery sensitive. We often warm up gate parameters, apply gradient clipping, and sometimes delay expert sparsity early in training to stabilize routing.â€

**Signal**

* This is a **research-meets-production** question

---

## â“ Q5 â€” PyTorch vs Custom Systems

**Question**

> *â€œWhat trade-offs did you face using native PyTorch distributed primitives versus more specialized MoE systems?â€*

**Expected answer**

> â€œPyTorch offers flexibility and composability, but specialized systems can optimize routing and communication more aggressively. PyTorch requires careful tuning to avoid abstraction overheads.â€

**Signal**

* Positions you as someone evaluating **build vs buy**

---

# ğŸ§  ONE-LINE INTRO YOU CAN USE BEFORE ASKING

This matters more than people think.

> â€œIâ€™ve been looking into scaling sparse models in PyTorch, especially around expert routing and distributed overheadâ€¦â€

That single sentence:

* Frames you as **peer**, not attendee
* Makes recruiters *look up*

---

# ğŸš€ How This Helps Your Visibility

* **Audience:** You sound calm, prepared, and precise
* **Speaker:** You ask something they *care* about
* **Recruiters:** You register as â€œsystems + MLâ€ talent

---

If you want next:

* I can compress this into a **1-page cheat sheet PDF**
* Or map these questions to **NVIDIA / Google / Meta recruiter signals**
* Or prep you for **follow-up hallway conversations**

Just say the word.




TO DO:
====
PUT SET PF QUESTIONS FOR EACH TOPICS TO CRATE VISIBILITY.
WRITE PROMPT TO ASK HW T GAIN VISIILITY
