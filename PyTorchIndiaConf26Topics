1.
Mixture-of-Experts (MoE) architectures offer a powerful path to scaling model capacity without proportional increases in compute â€” but training them efficiently at large scale has traditionally required complex distributed systems expertise. This talk walks through how developers can scale MoE models directly in PyTorch, using distributed features and optimized parallelism strategies


2.
This talk will focus on the role of compilers in the era of AI programming frameworks (e.g. PyTorch) and AI hardware accelerators. AI models are evolving and continue to rely on high-performance computing. Specialized hardware for AI is often hard to program to exploit peak performance. AI models are also evolving in a way that is coupled with hardware strengths. This talk will describe how to build effective compiler systems in a layered way to improve hardware usability and deliver high performance as automatically as possible.

3.
Problem Framing:
Speech is the most natural interface for humansâ€”but historically the hardest modality to scale. In multilingual regions like India, text-first AI systems struggle for deep penetration making way for speech as the default gateway to equitable AI access.

Core Thesis:
ASR has transitioned from acoustic-first pipelines to an LLM-first paradigm. Modern decoder-only Audio LLMs treat speech as another tokenized modality, unlocking better multilingual scaling, reasoning, and adaptation.

What This Talk Covers:
Evolution of ASR architectures:
CTC â†’ Encoderâ€“Decoder (with cross-attention) â†’ Decoder-only Audio LLMs
Why alignment (audio â†’ token space) is the key technical unlock
Tradeoffs across pradigmns: latency, streaming, robustness, multilinguality
Practical post-training strategies for domain- and language-specific ASR
Why It Matters:
This shift collapses the boundary between speech recognition and language understandingâ€”making speech a first-class citizen in foundation model stacks.

4.beyond-the-brrr-building-a-unified-ecosystem-for-optimized-kernels
As deep learning models grow in complexity, the "efficiency tax" of generic operators has become a critical bottleneck. While custom kernels offer the promise of hardware-saturating performance, making the hardware go "brrr", the reality of implementing them is fraught with challenges. Developers are often caught in a cycle of managing device-specific dependencies, handling complex compilation chains, and struggling with the lack of modularity when swapping kernels across different hardware backends.
In this talk, we bridge the gap between low-level optimization and high-level developer productivity. We will explore why optimized kernels are essential for modern workloads and dissect the friction points that make them difficult to maintain and distribute. We will introduce the new Kernels library from Hugging Face, discussing how it makes the experience of working with pre-built kernels a seamless and enjoyable experience. Participants will learn how to discover, use, and share optimized kernels as easily as model checkpoints. They will also see how Kernels is integrated inside the mighty Transformers library and how it helps in shipping latency improvements.

5.
Deploying massive Mixture-of-Experts (MoE) models is primarily constrained by memory bandwidth and KV-cache fragmentation. This session presents an optimized inference pipeline integrating vLLMâ€™s PagedAttention and continuous batching with the native hardware acceleration of the NVIDIA Blackwell architecture. We will demonstrate the implementation of native NVFP4 quantization on Blackwell Tensor Cores, analyzing how 4-bit floating-point precision on Blackwell mitigates memory bandwidth bottlenecks to accelerate tensor execution and maximize token throughput for sparse architectures.


===========
questions to pose:
======
Core Resources to Understand Topic #1 (MoE in PyTorch)
ğŸ§  1) Foundational Papers

â€œMixture of Experts with Shardingâ€ â€” Google / GShard (introduces MoE scaling)
â¤ Understand expert routing and conditional computation.

â€œSwitch Transformers: Scaling to Trillion Parameter Modelsâ€
â¤ One of the most influential MoE models.

â€œFast MoE: Efficient Sparsely Activated Transformersâ€
â¤ Optimization-oriented strategies relevant to PyTorch scaling.

These give the theory â€” gating, expert balancing, capacity factor, load balancing loss.

ğŸ›  2) PyTorch & MoE Training Tutorials

PyTorch MoE Example Repos

Facebook / PyTorch MoE implementations

Examples in torch.distributed (RPC based)

DeepSpeed MoE Tutorials

How ZeRO + MoE jointly scale training

Expert balancing & partitioning

DeepSpeed implementation details help bridge theory â†’ practical execution.

ğŸ§ª 3) Lecture / Video Resources

MIT Deep Learning Lecture: Efficient Models

Mixture of Experts intro from first principles

YouTube talks

â€œMoE breakdownâ€ sessions by Hugging Face, NVIDIA GTC talks

Practical PyTorch implementations

ğŸ“˜ 4) Blog / Conceptual Writeups

Distill Pub â€” MoE visual explanations

Great for intuition around how gates pick experts

Hugging Face & DeepSpeed blogs

Step-by-step training considerations

ğŸ” Key Concepts You Should Master Before the Talk

ğŸ“Œ Expert routing & gate design
ğŸ“Œ Load balancing & capacity factor
ğŸ“Œ Distributed training overheads
ğŸ“Œ Memory & communication bottlenecks
ğŸ“Œ Comparison with dense models

ğŸ§  5 High-Impact Questions to Ask at the Talk

These arenâ€™t generic â€” theyâ€™re crafted to show systems-level depth and practical engineering insight.

â“ Question 1 â€” Load Balancing

â€œHow does your implementation handle expert capacity imbalance at scale, and what load balancing losses or strategies do you use to prevent expert underutilization?â€

ğŸ’¡ Why itâ€™s impressive
MoE systems break if experts arenâ€™t balanced â€” this is where many go wrong.

âœ… Strong Answer

â€œWe use a combination of auxiliary load-balancing loss terms and capacity factors. By using a softmax gating with additional balancing regularizers, we discourage the gate from collapsing to a few experts. In PyTorch distributed, we also shard experts so that each device holds multiple experts, which reduces imbalance across machines.â€

â“ Question 2 â€” Communication Overhead

â€œIn your distributed design, where do you see most of the communication overhead â€” expert updates or routing â€” and how do you mitigate that?â€

ğŸ’¡ Why itâ€™s impressive
Recruiters want to see you think about the hidden cost of distribution.

âœ… Strong Answer

â€œThe dominant overhead usually comes from all-to-all communication during expert dispatch. To mitigate this, the implementation balances local expert replicas and uses efficient collective operations. When possible, we fuse communication with forward/backward passes, reducing transfer overhead between GPUs.â€

â“ Question 3 â€” Scalability Limits

â€œAt what scale (experts Ã— batch size Ã— devices) did you observe diminishing returns, and what were the system bottlenecks?â€

ğŸ’¡ Why itâ€™s impressive
Shows you think about engineering ceilings, not just theory.

âœ… Strong Answer

â€œWe saw diminishing returns when communication and KV cache overheads exceeded compute cost â€” around 128 experts with small batch sizes. At that point, routing overhead dominates. The threshold depends on batch size, expert count, and the quality of the collective backend (NCCL / Gloo).â€

â“ Question 4 â€” Model Accuracy Stability

â€œDid you observe any pattern where increasing experts hurt convergence stability, and if so, what mitigation strategies did you adopt?â€

ğŸ’¡ Why itâ€™s impressive
Connects system scaling with training quality.

âœ… Strong Answer

â€œYes â€” as expert count grows, the gate can become unstable. We used warm-up schedules for gate weights, gradient clipping on gating parameters, and occasionally auxiliary losses to stabilize routes. Normalizing expert outputs also helped.â€

â“ Question 5 â€” Memory & Gradient Overheads

â€œWhat memory optimizations did you require in PyTorch â€” like gradient checkpointing or expert sharding â€” to handle tens of millions of parameters?â€

ğŸ’¡ Why itâ€™s impressive
Shows you care about actual capacity limits.

âœ… Strong Answer

â€œWe heavily rely on expert sharding and checkpointing for intermediate activations. By sharding expert weights across devices, we reduce peak memory. Checkpointing the expert forward pass saves activation memory at the cost of recompute, which was balanced with batch size selection.â€

ğŸ“Œ Bonus: 3 Things to Say Before Asking Your Question

You want to frame yourself as a peer â€” not someone fishing for an answer.

Try these openers:

Before Q1

â€œIâ€™ve seen load imbalance crash MoE training at scale â€” Iâ€™m curious how you approach it.â€

Before Q2

â€œIn my experience with distributed workloads, communication trumps compute â€” how do you see it in MoE?â€

Before Q5

â€œMemory fragmentation becomes critical when sharding â€” have you run into that?â€

ğŸ“ˆ How This Positions You to Stakeholders

âœ” Recruiters: You immediately sound like someone familiar with real system constraints
âœ” Engineering Leaders: You show systems maturity, not just model theory
âœ” Open-Source Maintainers: You ask questions that could inform future frameworks




TO DO:
====
PUT SET PF QUESTIONS FOR EACH TOPICS TO CRATE VISIBILITY.
WRITE PROMPT TO ASK HW T GAIN VISIILITY
