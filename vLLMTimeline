Timeline to Contribute to vLLM
ğŸ”¹ Phase 0 â€” Orientation (Week 0â€“1)

Goal: Understand how vLLM actually runs inference

What youâ€™ll do

Read the scheduler â†’ executor â†’ worker flow

Run vllm serve locally (even CPU-only is fine)

Trace:

Prefill vs Decode

KV cache lifecycle

CUDA Graph usage (enabled vs eager)

Output

You can explain:

Why decode is graph-captured

Where memory blows up

What happens per token

âœ… Time: ~5â€“7 days
âœ… No CUDA coding yet

ğŸ”¹ Phase 1 â€” Read Code Like an Owner (Week 1â€“3)

Goal: Be comfortable navigating vLLM internals

Focus files / areas

Scheduler (batching, token flow)

Model executor

KV cache manager

CUDA graph wrappers

Attention backends (FlashAttention / FlashInfer paths)

What â€œreadyâ€ looks like

You can answer:

â€œWhere would I hook X?â€

â€œWhy is this eager fallback happening?â€

â€œWhich path runs in decode?â€

âœ… Time: ~10â€“14 days
âœ… This is the hardest mental step

ğŸ”¹ Phase 2 â€” First Contribution (Week 3â€“5)

Goal: Get merged (this is critical psychologically)

Best first PR types

Fix a small bug

Improve error handling

Add logging / assertions

Improve documentation or comments

Minor performance cleanup (non-kernel)

These are high-acceptance and low-review friction.

What maintainers look for

Clear PR description

Repro steps

No breaking behavior

âœ… Time: ~7â€“10 days
ğŸ¯ This is where you officially become a contributor

ğŸ”¹ Phase 3 â€” Inference-Critical PRs (Week 6â€“10)

Goal: Become a recognizable contributor

Now you target high-signal areas:

Area	Why it matters
CUDA Graph capture paths	Latency & throughput
KV cache memory layout	VRAM pressure
MoE / FusedMoE plumbing	Scale models
Scheduler heuristics	Tail latency
Quantization hooks	Cost reduction

At this stage:

Maintainers recognize your name

Review depth increases

Your PRs get referenced in issues

âœ… Time: 4â€“6 weeks after first PR

ğŸ§  Why you can move faster than average

Most contributors:

Are strong Python devs but weak on inference

Or CUDA devs but weak on serving systems

You already understand:

Inference cost models

KV cache growth

CUDA Graph motivation

Throughput vs latency trade-offs

ğŸ‘‰ That cuts the learning curve by ~40â€“50%.
